Machine Learning for Beginners: An Introduction to Neural Networks - victorzhou.comVictor ZhouBlogAboutTagsMore PostsMachine Learning for Beginners: An Introduction to Neural NetworksA simple explanation of how they work and how to implement one from scratch in Python.March 3, 2019 | UPDATED September 16, 2022Here’s something that might surprise you: neural networks aren’t that complicated! The term “neural network” gets used as a buzzword a lot, but in reality they’re often much simpler than people imagine. This post is intended for complete beginners and assumes ZERO prior knowledge of machine learning. We’ll understand how neural networks work while implementing one from scratch in Python. Let’s get started! 1. Building Blocks: Neurons First, we have to talk about neurons, the basic unit of a neural network. A neuron takes inputs, does some math with them, and produces one output. Here’s what a 2-input neuron looks like: 3 things are happening here. First, each input is multiplied by a weight: x1→x1∗w1x_1 \rightarrow x_1 * w_1x1​→x1​∗w1​ x2→x2∗w2x_2 \rightarrow x_2 * w_2x2​→x2​∗w2​ Next, all the weighted inputs are added together with a bias bbb: (x1∗w1)+(x2∗w2)+b(x_1 * w_1) + (x_2 * w_2) + b(x1​∗w1​)+(x2​∗w2​)+b Finally, the sum is passed through an activation function: y=f(x1∗w1+x2∗w2+b)y = f(x_1 * w_1 + x_2 * w_2 + b)y=f(x1​∗w1​+x2​∗w2​+b) The activation function is used to turn an unbounded input into an output that has a nice, predictable form. A commonly used activation function is the sigmoid function: The sigmoid function only outputs numbers in the range (0,1)(0, 1)(0,1). You can think of it as compressing (−∞,+∞)(-\infty, +\infty)(−∞,+∞) to (0,1)(0, 1)(0,1) - big negative numbers become ~000, and big positive numbers become ~111. A Simple Example Assume we have a 2-input neuron that uses the sigmoid activation function and has the following parameters: w=[0,1]w = [0, 1]w=[0,1] b=4b = 4b=4 w=[0,1]w = [0, 1]w=[0,1] is just a way of writing w1=0,w2=1w_1 = 0, w_2 = 1w1​=0,w2​=1 in vector form. Now, let’s give the neuron an input of x=[2,3]x = [2, 3]x=[2,3]. We’ll use the dot product to write things more concisely: (w⋅x)+b=((w1∗x1)+(w2∗x2))+b=0∗2+1∗3+4=7\begin{aligned} (w \cdot x) + b &= ((w_1 * x_1) + (w_2 * x_2)) + b \\ &= 0 * 2 + 1 * 3 + 4 \\ &= 7 \\ \end{aligned}(w⋅x)+b​=((w1​∗x1​)+(w2​∗x2​))+b=0∗2+1∗3+4=7​ y=f(w⋅x+b)=f(7)=0.999y = f(w \cdot x + b) = f(7) = \boxed{0.999}y=f(w⋅x+b)=f(7)=0.999​ The neuron outputs 0.9990.9990.999 given the inputs x=[2,3]x = [2, 3]x=[2,3]. That’s it! This process of passing inputs forward to get an output is known as feedforward. Coding a Neuron Time to implement a neuron! We’ll use NumPy, a popular and powerful computing library for Python, to help us do math: import numpy as np def sigmoid(x): # Our activation function: f(x) = 1 / (1 + e^(-x)) return 1 / (1 + np.exp(-x)) class Neuron: def __init__(self, weights, bias): self.weights = weights self.bias = bias def feedforward(self, inputs): # Weight inputs, add bias, then use the activation function total = np.dot(self.weights, inputs) + self.bias return sigmoid(total) weights = np.array([0, 1]) # w1 = 0, w2 = 1 bias = 4 # b = 4 n = Neuron(weights, bias) x = np.array([2, 3]) # x1 = 2, x2 = 3 print(n.feedforward(x)) # 0.9990889488055994 Recognize those numbers? That’s the example we just did! We get the same answer of 0.9990.9990.999. 2. Combining Neurons into a Neural Network A neural network is nothing more than a bunch of neurons connected together. Here’s what a simple neural network might look like: This network has 2 inputs, a hidden layer with 2 neurons (h1h_1h1​ and h2h_2h2​), and an output layer with 1 neuron (o1o_1o1​). Notice that the inputs for o1o_1o1​ are the outputs from h1h_1h1​ and h2h_2h2​ - that’s what makes this a network. A hidden layer is any layer between the input (first) layer and output (last) layer. There can be multiple hidden layers! An Example: Feedforward Let’s use the network pictured above and assume all neurons have the same weights w=[0,1]w = [0, 1]w=[0,1], the same bias b=0b = 0b=0, and the same sigmoid activation function. Let h1,h2,o1h_1, h_2, o_1h1​,h2​,o1​ denote the outputs of the neurons they represent. What happens if we pass in the input x=[2,3]x = [2, 3]x=[2,3]? h1=h2=f(w⋅x+b)=f((0∗2)+(1∗3)+0)=f(3)=0.9526\begin{aligned} h_1 = h_2 &= f(w \cdot x + b) \\ &= f((0 * 2) + (1 * 3) + 0) \\ &= f(3) \\ &= 0.9526 \\ \end{aligned}h1​=h2​​=f(w⋅x+b)=f((0∗2)+(1∗3)+0)=f(3)=0.9526​ o1=f(w⋅[h1,h2]+b)=f((0∗h1)+(1∗h2)+0)=f(0.9526)=0.7216\begin{aligned} o_1 &= f(w \cdot [h_1, h_2] + b) \\ &= f((0 * h_1) + (1 * h_2) + 0) \\ &= f(0.9526) \\ &= \boxed{0.7216} \\ \end{aligned}o1​​=f(w⋅[h1​,h2​]+b)=f((0∗h1​)+(1∗h2​)+0)=f(0.9526)=0.7216​​ The output of the neural network for input x=[2,3]x = [2, 3]x=[2,3] is 0.72160.72160.7216. Pretty simple, right? A neural network can have any number of layers with any number of neurons in those layers. The basic idea stays the same: feed the input(s) forward through the neurons in the network to get the output(s) at the end. For simplicity, we’ll keep using the network pictured above for the rest of this post. Coding a Neural Network: Feedforward Let’s implement feedforward for our neural network. Here’s the image of the network again for reference: import numpy as np # ... code from previous section here class OurNeuralNetwork: ''' A neural network with: - 2 inputs - a hidden layer with 2 neurons (h1, h2) - an output layer with 1 neuron (o1) Each neuron has the same weights and bias: - w = [0, 1] - b = 0 ''' def __init__(self): weights = np.array([0, 1]) bias = 0 # The Neuron class here is from the previous section self.h1 = Neuron(weights, bias) self.h2 = Neuron(weights, bias) self.o1 = Neuron(weights, bias) def feedforward(self, x): out_h1 = self.h1.feedforward(x) out_h2 = self.h2.feedforward(x) # The inputs for o1 are the outputs from h1 and h2 out_o1 = self.o1.feedforward(np.array([out_h1, out_h2])) return out_o1 network = OurNeuralNetwork() x = np.array([2, 3]) print(network.feedforward(x)) # 0.7216325609518421 We got 0.72160.72160.7216 again! Looks like it works. Liking this post so far? Subscribe to my newsletter to get more ML content in your inbox. 3. Training a Neural Network, Part 1 Say we have the following